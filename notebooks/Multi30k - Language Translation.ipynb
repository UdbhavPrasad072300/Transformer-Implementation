{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_is_mode_type\u001b[1;34m(path, mode)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified: 'c:\\\\users\\\\udbhav prasad\\\\anaconda3\\\\lib\\\\site-packages\\\\google\\\\__init__.py'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-70bf3038da5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchsummary\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msummary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mthinc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneural\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprefer_gpu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire_gpu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mcli\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcli_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mglossary\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexplain\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\pipeline\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0municode_literals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpipes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTagger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDependencyParser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEntityRecognizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEntityLinker\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpipes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextCategorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPipe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSentencizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmorphologizer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMorphologizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpipes.pyx\u001b[0m in \u001b[0;36minit spacy.pipeline.pipes\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mnn_parser.pyx\u001b[0m in \u001b[0;36minit spacy.syntax.nn_parser\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mstrings.pyx\u001b[0m in \u001b[0;36minit spacy.strings\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[1;32mimport\u001b[0m \u001b[0mjsonschema\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mjsonschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\jsonschema\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m )\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpkg_resources\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_distribution\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[0m__version__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_distribution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m   3241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3242\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0m_call_aside\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3243\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0m_initialize_master_working_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3244\u001b[0m     \"\"\"\n\u001b[0;32m   3245\u001b[0m     \u001b[0mPrepare\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmaster\u001b[0m \u001b[0mworking\u001b[0m \u001b[0mset\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmake\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mrequire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36m_call_aside\u001b[1;34m(f, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3224\u001b[0m \u001b[1;31m# from jaraco.functools 1.3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3225\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_call_aside\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3226\u001b[1;33m     \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3227\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36m_initialize_master_working_set\u001b[1;34m()\u001b[0m\n\u001b[0;32m   3266\u001b[0m     \u001b[1;31m# (e.g. by calling ``require()``) will get activated as well,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3267\u001b[0m     \u001b[1;31m# with higher priority (replace=True).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3268\u001b[1;33m     tuple(\n\u001b[0m\u001b[0;32m   3269\u001b[0m         \u001b[0mdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3270\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mworking_set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   3267\u001b[0m     \u001b[1;31m# with higher priority (replace=True).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3268\u001b[0m     tuple(\n\u001b[1;32m-> 3269\u001b[1;33m         \u001b[0mdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3270\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mworking_set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3271\u001b[0m     )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36mactivate\u001b[1;34m(self, path, replace)\u001b[0m\n\u001b[0;32m   2778\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert_on\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2779\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2780\u001b[1;33m             \u001b[0mfixup_namespace_packages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2781\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mpkg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'namespace_packages.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2782\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mpkg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36mfixup_namespace_packages\u001b[1;34m(path_item, parent)\u001b[0m\n\u001b[0;32m   2291\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2292\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mpackage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_namespace_packages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2293\u001b[1;33m             \u001b[0msubpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_handle_ns\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpackage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_item\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2294\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msubpath\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2295\u001b[0m                 \u001b[0mfixup_namespace_packages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36m_handle_ns\u001b[1;34m(packageName, path_item)\u001b[0m\n\u001b[0;32m   2192\u001b[0m     \u001b[1;31m# use find_spec (PEP 451) and fall-back to find_module (PEP 302)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2193\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2194\u001b[1;33m         \u001b[0mloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimporter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_spec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpackageName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2195\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2196\u001b[0m         \u001b[1;31m# capture warnings due to #1111\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_isfile\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_is_mode_type\u001b[1;34m(path, mode)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "\n",
    "import torchtext\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import spacy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "from PIL import Image\n",
    "import glob\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "LR = 3e-4\n",
    "NUM_EPOCHES = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_english = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_german = spacy.load(\"de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_english(text):\n",
    "    return [token.text for token in nlp_english.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_german(text):\n",
    "    return [token.text for token in nlp_german.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_english(\"Hi guys, my name Jeff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_german(\"I dont know any German\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGLISH = torchtext.data.Field(tokenize=tokenizer_english, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GERMAN = torchtext.data.Field(tokenize=tokenizer_german, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation, test = torchtext.datasets.Multi30k.splits(exts=(\".de\", \".en\"), fields=(GERMAN, ENGLISH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGLISH.build_vocab(train, max_size=10000, min_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GERMAN.build_vocab(train, max_size=10000, min_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ENGLISH vocab_size: \", len(ENGLISH.vocab))\n",
    "print(\"GERMAN vocab_size: \", len(GERMAN.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, validation_dataloader, test_dataloader = torchtext.data.BucketIterator.splits(\n",
    "    (train, validation, test),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x.src),\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, data in enumerate(train_dataloader):\n",
    "    print(batch_idx)\n",
    "    print(data.src.size())\n",
    "    print(data.trg.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def german2english(model, german_sentence, device=\"cpu\", max_len=100):\n",
    "    model.eval()\n",
    "    tokens = [token.text.lower() for token in nlp_german(german_sentence)]\n",
    "    tokens = [\"<sos>\"] + tokens + [\"<eos>\"]\n",
    "    \n",
    "    indexes = [GERMAN.vocab.stoi[token] for token in tokens]\n",
    "    indexes_tensor = torch.LongTensor(indexes).unsqueeze(1).to(device)\n",
    "    \n",
    "    english_sentence = [ENGLISH.vocab.stoi[\"<sos>\"]]\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        trg = torch.LongTensor(english_sentence).unsqueeze(1).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            word = model(indexes_tensor, trg)\n",
    "            \n",
    "        top = word.argmax(-1)[-1, :].item()\n",
    "        english_sentence.append(top)\n",
    "\n",
    "        if top == ENGLISH.vocab.stoi[\"<eos>\"]:\n",
    "            break\n",
    "\n",
    "    english_sentence = [ENGLISH.vocab.itos[word] for word in english_sentence]\n",
    "    \n",
    "    return english_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGLISH.vocab.stoi[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_package.models.transformer import Transformer, Transformer_with_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_vocab_size = len(GERMAN.vocab)\n",
    "target_vocab_size = len(ENGLISH.vocab)\n",
    "embed_size = 512\n",
    "num_head = 8\n",
    "num_ff = 1024\n",
    "encoder_layers = 3\n",
    "decoder_layers = 3\n",
    "hidden_size = 512\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(source_vocab_size, target_vocab_size, embed_size, num_head, num_ff, encoder_layers, decoder_layers, hidden_size, dropout=dropout, device=device).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(size):\n",
    "    sample_in_x = torch.rand(100, BATCH_SIZE).type(torch.LongTensor).to(device)\n",
    "    sample_in_y = torch.rand(size, BATCH_SIZE).type(torch.LongTensor).to(device)\n",
    "    sample_out = model(sample_in_x, sample_in_y)\n",
    "    print(\"Dimenstions of Input Source Vector: \", sample_in_x.size())\n",
    "    print(\"Dimenstions of Input Target Vector: \", sample_in_y.size())\n",
    "    print(\"Dimenstions of Predicted Vector: \", sample_out.size())\n",
    "    \n",
    "test(90)\n",
    "test(100)\n",
    "test(110)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = ENGLISH.vocab.stoi[\"<pad>\"])\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_hist = {}\n",
    "loss_hist[\"train loss\"] = []\n",
    "loss_hist[\"test loss\"] = []\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHES+1):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_train_loss = 0\n",
    "    epoch_test_loss = 0  \n",
    "    \n",
    "    for batch_idx, data in enumerate(train_dataloader):\n",
    "        x = data.src.to(device)\n",
    "        y = data.trg.to(device)\n",
    "        \n",
    "        y_pred = model(x, y[:-1, :])\n",
    "        \n",
    "        y_pred = y_pred.reshape(-1, y_pred.size(-1))\n",
    "        y = y[1:, :].reshape(-1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(y_pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_train_loss += loss.item()\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        \n",
    "        for batch_idx, data in enumerate(validation_dataloader):\n",
    "            x = data.src.to(device)\n",
    "            y = data.trg.to(device)\n",
    "        \n",
    "            y_pred = model(x, y[:-1, :])\n",
    "            y_pred = y_pred.reshape(-1, y_pred.size(-1))\n",
    "            y = y[1:, :].reshape(-1)\n",
    "        \n",
    "            loss = criterion(y_pred, y)\n",
    "        \n",
    "            epoch_test_loss += loss.item()\n",
    "    \n",
    "    epoch_train_loss = epoch_train_loss / len(train_dataloader.dataset)\n",
    "    epoch_test_loss = epoch_test_loss / len(validation_dataloader.dataset)\n",
    "    \n",
    "    loss_hist[\"train loss\"].append(epoch_train_loss)\n",
    "    loss_hist[\"test loss\"].append(epoch_test_loss)\n",
    "    \n",
    "    if epoch%1 == 0:\n",
    "        print(german2english(model, \"Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.\", device=device)) # Two young, White males are outside near many bushes.\n",
    "        print(\"-------------------------------------------------\")\n",
    "        print(\"Epoch: {} Train mean loss: {:.8f}\".format(epoch, epoch_train_loss))\n",
    "        print(\"       {} Test  mean loss: {:.8f}\".format(epoch, epoch_test_loss))\n",
    "        print(\"-------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_hist[\"train loss\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_hist[\"test loss\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german2english(model, \"Ein Mann mit einem orangefarbenen Hut, der etwas anstarrt.\", device=device) # A man in an orange hat starring at something."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model, \"trained_models/language_translation_1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
